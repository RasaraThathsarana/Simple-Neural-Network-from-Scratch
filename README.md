# Simple Neural Network from Scratch

This project involves implementing a general neural network from scratch without relying on external neural network libraries. The network is built to classify data with 14 binary attributes into four classes, using backpropagation and gradient descent. ReLU activation is used in the hidden layers, while the Softmax function with cross-entropy loss is applied in the output layer. The neural network is designed to train efficiently across different tasks beyond this assignment.
